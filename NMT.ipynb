{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxTM2E3pEfIayW6ncdBrb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samyxandz/NMT-English-To-Portugeese/blob/main/NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation\n",
        "\n",
        "English-to-Portuguese neural machine translation (NMT) model using Long Short-Term Memory (LSTM) networks with attention.\n",
        "\n",
        "Implementing this using just a Recurrent Neural Network (RNN) with LSTMs can work for short to medium length sentences but can result in vanishing gradients for very long sequences. To help with this, we will be adding an attention mechanism to allow the decoder to access all relevant parts of the input sentence regardless of its length.\n",
        "\n",
        "\n",
        "## Following are the main steps in the program:\n",
        "- Implement an encoder-decoder system with attention\n",
        "- Build the NMT model from scratch using Tensorflow\n",
        "- Generate translations using greedy and Minimum Bayes Risk (MBR) decoding\n"
      ],
      "metadata": {
        "id": "Xe27ka84i-BZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing the required libraries"
      ],
      "metadata": {
        "id": "NjstUDShFoDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy tensorflow tensorflow_text"
      ],
      "metadata": {
        "id": "tWqWXZI8FmCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a24684d9-ce2a-4012-87b7-9ccff74828eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow_text) (2.15.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow_text\n",
            "Successfully installed tensorflow_text-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ah7Sj-96iSk_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import pathlib\n",
        "\n",
        "\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "path_to_file = pathlib.Path(\"por-eng/por.txt\")#/content/por-eng/por.txt"
      ],
      "metadata": {
        "id": "86eLcM_0H0HN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "hEBr1Lpi3c5p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding helper functions\n"
      ],
      "metadata": {
        "id": "of5Q2gX6JntH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data(path):\n",
        "    text = path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    lines = text.splitlines()\n",
        "    pairs = [line.split(\"\\t\") for line in lines]\n",
        "\n",
        "    context = np.array([context for target, context, _ in pairs])\n",
        "    target = np.array([target for target, context, _ in pairs])\n",
        "\n",
        "    return context, target\n"
      ],
      "metadata": {
        "id": "cICUk0koICB5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portuguese_sentences, english_sentences = load_data(path_to_file)\n",
        "sentences = (portuguese_sentences, english_sentences)"
      ],
      "metadata": {
        "id": "hNCewP1Ne7S5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did u know ?\n",
        "\n",
        "Due to some problem the above code doent work on a cpu environment,\n",
        "\n",
        "but when swited to a gpu environmnet it did\n",
        "\n",
        "may be coz it had 19000+ line ☠"
      ],
      "metadata": {
        "id": "JP87dX5_bShS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing wrapper functions"
      ],
      "metadata": {
        "id": "K7SeJc5F-qky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dividing into batches"
      ],
      "metadata": {
        "id": "DN8gWJAbqTCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(english_sentences)\n",
        "BATCH_SIZE = 64\n",
        "is_train = np.random.uniform(size=(len(portuguese_sentences),)) < 0.8\n",
        "\n",
        "train_raw = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (english_sentences[is_train], portuguese_sentences[is_train])\n",
        "    )\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "val_raw = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (english_sentences[~is_train], portuguese_sentences[~is_train])\n",
        "    )\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "Hs_KDEe7qEnc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalising the Text"
      ],
      "metadata": {
        "id": "7Bvv8Cyk3cpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n",
        "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
        "    return text"
      ],
      "metadata": {
        "id": "MaeQZfU65HPF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 12000"
      ],
      "metadata": {
        "id": "YiF7ioVC50Rf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Vectorizing the Sentences"
      ],
      "metadata": {
        "id": "3Iia4Wtt50D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorizer = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True)\n",
        "english_vectorizer.adapt(train_raw.map(lambda context, target: context))\n",
        "\n",
        "portuguese_vectorizer = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True)\n",
        "portuguese_vectorizer.adapt(train_raw.map(lambda context, target: target))"
      ],
      "metadata": {
        "id": "I0z1GNdq6iGl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Text Processor wrapper"
      ],
      "metadata": {
        "id": "axDE_0GT8j5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(context, target):\n",
        "    context = english_vectorizer(context).to_tensor()\n",
        "    target = portuguese_vectorizer(target)\n",
        "    targ_in = target[:, :-1].to_tensor()\n",
        "    targ_out = target[:, 1:].to_tensor()\n",
        "    return (context, targ_in), targ_out"
      ],
      "metadata": {
        "id": "q3hQPtlU8jri"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "checking the above functions"
      ],
      "metadata": {
        "id": "_rGZ1POUmQpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_data = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "k-YSBY3TXSJf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
        "print(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")"
      ],
      "metadata": {
        "id": "tLnB67aq_Pyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbebb396-afa3-4b82-f217-497e71788837"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English (to translate) sentence:\n",
            "\n",
            "No matter how much you try to convince people that chocolate is vanilla, it'll still be chocolate, even though you may manage to convince yourself and a few others that it's vanilla.\n",
            "\n",
            "Portuguese (translation) sentence:\n",
            "\n",
            "Não importa o quanto você tenta convencer os outros de que chocolate é baunilha, ele ainda será chocolate, mesmo que você possa convencer a si mesmo e poucos outros de que é baunilha.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First 10 words of the english vocabulary:\\n\\n{english_vectorizer.get_vocabulary()[:10]}\\n\")\n",
        "print(f\"First 10 words of the portuguese vocabulary:\\n\\n{portuguese_vectorizer.get_vocabulary()[:10]}\")"
      ],
      "metadata": {
        "id": "7xdV5sBH_ljz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b859e3b-e1c0-4f2d-ba83-7a8060b35bf4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 words of the english vocabulary:\n",
            "\n",
            "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'i', 'to', 'you', 'the']\n",
            "\n",
            "First 10 words of the portuguese vocabulary:\n",
            "\n",
            "['', '[UNK]', '[SOS]', '[EOS]', '.', 'tom', 'que', 'o', 'nao', 'eu']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fg51wGjFgC1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = portuguese_vectorizer.vocabulary_size()"
      ],
      "metadata": {
        "id": "Io0G48_R_84E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Using [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) objects help map from words to ids and vice versa."
      ],
      "metadata": {
        "id": "bCWzyLu6ABqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_id = tf.keras.layers.StringLookup(\n",
        "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
        "    mask_token=\"\",\n",
        "    oov_token=\"[UNK]\"\n",
        ")\n",
        "\n",
        "\n",
        "id_to_word = tf.keras.layers.StringLookup(\n",
        "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
        "    mask_token=\"\",\n",
        "    oov_token=\"[UNK]\",\n",
        "    invert=True,\n",
        ")"
      ],
      "metadata": {
        "id": "tNCjQQJk_9zr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_id = word_to_id(\"[UNK]\")\n",
        "sos_id = word_to_id(\"[SOS]\")\n",
        "eos_id = word_to_id(\"[EOS]\")\n",
        "baunilha_id = word_to_id(\"baunilha\")"
      ],
      "metadata": {
        "id": "l-BC6UZQgDFS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (to_translate, sr_translation), translation in train_data.take(1):\n",
        "    print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
        "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
        "    print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcFztvGlW4it",
        "outputId": "5fa720c8-e94e-4335-e8a9-306c5da684be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized english sentence:\n",
            "[   2  210    9  146  123   38    9 1672    4    3    0    0    0    0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence (shifted to the right):\n",
            "[   2 1085    7  128   11  389   37 2038    4    0    0    0    0    0\n",
            "    0]\n",
            "\n",
            "\n",
            "Tokenized portuguese sentence:\n",
            "[1085    7  128   11  389   37 2038    4    3    0    0    0    0    0\n",
            "    0]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMT model with attention\n",
        "\n",
        "\n",
        "*   The model you will uses an encoder-decoder architecture.\n"
      ],
      "metadata": {
        "id": "poe3tcdqBqBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This **Recurrent Neural Network (RNN)** takes in a tokenized version of a sentence in its *encoder*, then passes it on to the *decoder* for translation.\n",
        "\n",
        "You can picture it like the figure below where all of the context of the input sentence **is compressed into one vector** that is passed into the decoder block.\n",
        "You can see how this will be an issue for very long sentences (e.g. 100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.\n",
        "\n",
        "<img src='/images/plain_rnn.png'>\n",
        "\n",
        "\n",
        "Adding an **attention layer** to this model avoids this problem by giving the decoder access to all parts of the input sentence.\n",
        "\n",
        "To illustrate, let's just use a 4-word input sentence as shown below. These are all passed to the attention layer and each are given a score given the current activation (i.e. hidden state) of the decoder.\n",
        "\n",
        "To produce the next prediction, the attention layer will first receive all the encoder hidden states as well as the decoder hidden state when producing the word \"como\" .\n",
        "\n",
        " Given this information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word. As a result of training, the model might have learned that it should align to the second encoder hidden state and subsequently assigns a high probability to the word \"você\".\n",
        "\n",
        "<img src='images/attention_overview.png'>\n"
      ],
      "metadata": {
        "id": "nCEs2A0xteM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different ways to implement attention and the one we'll use for is the Scaled Dot Product Attention which has the form:\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        " This context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word. The division by square root of the keys dimensionality ($\\sqrt{d_k}$) is for improving model performance and you'll also learn more about it next week."
      ],
      "metadata": {
        "id": "9QHTcSUbgPoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 12000\n",
        "UNITS = 256"
      ],
      "metadata": {
        "id": "mHX7-DczXg-S"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder\n",
        "Making a c**ustom Encoder Layer**\n",
        "\n",
        "The encoder consists of the following layers:\n",
        "\n",
        "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
        "    \n",
        "+ [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): bidirectional behaviour for RNN-like layers.   \n",
        "  \n"
      ],
      "metadata": {
        "id": "s4HWBqI5dz5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=units,\n",
        "            mask_zero=True\n",
        "        )\n",
        "\n",
        "        self.rnn = tf.keras.layers.Bidirectional(\n",
        "            merge_mode=\"sum\",\n",
        "            layer=tf.keras.layers.LSTM(\n",
        "                units=units,\n",
        "                return_sequences=True\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(self, context):\n",
        "\n",
        "        x = self.embedding(context)\n",
        "        x = self.rnn(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "XqHw-TlJefWH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing if any breaks\n",
        "encoder = Encoder(VOCAB_SIZE, UNITS)"
      ],
      "metadata": {
        "id": "eSTKiBSzXhNu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Attention"
      ],
      "metadata": {
        "id": "LWwaGh-qtQVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cross attention consists of the following layers:\n",
        "\n",
        "- [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). The reason why this layer is preferred over [Attention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention) is that it allows simpler code during the forward pass.\n",
        "    \n",
        "\n",
        "- Need to pass both the output of the attention alongside the shifted-to-the-right translation (since this cross attention happens in the decoder side). For this you will use an [Add](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add) layer so that the original dimension is preserved\n",
        "+ Layer normalization is also performed for better stability of the network by using  [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization) .\n"
      ],
      "metadata": {
        "id": "VYTDZV0Kx9Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.mha = (\n",
        "            tf.keras.layers.MultiHeadAttention(\n",
        "                key_dim=units,\n",
        "                num_heads=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "    def call(self, context, target):\n",
        "        attn_output = self.mha(\n",
        "            query=target,\n",
        "            value=context\n",
        "        )\n",
        "\n",
        "        x = self.add([target, attn_output])\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "FCRDxqHRx9I8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing if any breaks\n",
        "attention_layer = CrossAttention(UNITS)"
      ],
      "metadata": {
        "id": "jMQoWqBlXqOv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder\n"
      ],
      "metadata": {
        "id": "dS_av7t-1UBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The decoder consists of the following layers:\n",
        "\n",
        "- [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)\n",
        "  \n",
        "  \n",
        "+ Pre-attention [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
        "\n",
        "  *   A vanilla LSTM\n",
        "  *   The LSTM returns the full sequence and not only the last output\n",
        "  *   It is very important that this layer returns the state since this will be needed for inference\n",
        "\n",
        "- The attention layer that performs cross attention between the sentence to translate and the right-shifted translation. Here you need to use the `CrossAttention` layer you defined in the previous exercise.\n",
        "\n",
        "+ Post-attention [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
        "\n",
        "- [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer.\n",
        " Make sure to use a `logsoftmax` activation function for this one, which you\n",
        "\n",
        "  *   Make sure to use a `logsoftmax` activation function for this one, which you\n",
        "  can get as [tf.nn.log_softmax](https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax).\n",
        "\n"
      ],
      "metadata": {
        "id": "yr697eR_yK9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding =tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=units,\n",
        "            mask_zero=True\n",
        "        )\n",
        "        # The RNN before attention\n",
        "        self.pre_attention_rnn = tf.keras.layers.LSTM(\n",
        "            units=units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "        self.attention = CrossAttention(units=units)\n",
        "\n",
        "        # The RNN after attention\n",
        "        self.post_attention_rnn = tf.keras.layers.LSTM(\n",
        "            units=units,\n",
        "            return_sequences=True\n",
        "        )\n",
        "\n",
        "        # The dense layer with logsoftmax activation\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            units=vocab_size,\n",
        "            activation=tf.nn.log_softmax\n",
        "        )\n",
        "\n",
        "    def call(self, context, target, state=None, return_state=False):\n",
        "        x = self.embedding(target)\n",
        "        x, hidden_state, cell_state = self.pre_attention_rnn(x, initial_state=state)\n",
        "\n",
        "        x = self.attention(context, x)\n",
        "        x = self.post_attention_rnn(x)\n",
        "\n",
        "        # Compute the logits\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        if return_state:\n",
        "            return logits, [hidden_state, cell_state]\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "0qTTBoCMyLK7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(VOCAB_SIZE, UNITS)"
      ],
      "metadata": {
        "id": "LaUZPT0ygVLy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translator\n",
        "\n",
        "Putting together all of the layers you previously coded into an actual model.\n",
        "\n",
        " The Translator class inherits from `tf.keras.layers.Model`.\n",
        "\n",
        "Remember that `train_data` will yield a tuple with the sentence to translate and the shifted-to-the-right translation, which are the \"features\" of the model."
      ],
      "metadata": {
        "id": "7ZfTBlHC3aUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, units):\n",
        "\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size=vocab_size, units=units)\n",
        "        self.decoder = Decoder(vocab_size=vocab_size, units=units)\n",
        "    def call(self, inputs):\n",
        "        context, target = inputs\n",
        "        encoded_context= self.encoder(context)\n",
        "\n",
        "        logits = self.decoder(encoded_context,target)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "qExxKmWg3mgM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(VOCAB_SIZE, UNITS)\n",
        "logits = translator((to_translate, sr_translation))"
      ],
      "metadata": {
        "id": "nW2ayJTFWSId"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "XlQa-uBR80EY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "took a long time man\n",
        "\n",
        "  Also Hi there !\n",
        "\n",
        "  If u reading this come say Hi at discord(samyxandy) or [LinkedIn](https://www.linkedin.com/in/tamaghna-choudhuri/) or [Github](https://github.com/samyxandz)"
      ],
      "metadata": {
        "id": "XYlQuXCtvr3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "    # Check which elements of y_true are padding\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "\n",
        "    loss *= mask\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def masked_acc(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def tokens_to_text(tokens, id_to_word):\n",
        "    words = id_to_word(tokens)\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=\" \")\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "xAgsIURySVxo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_and_train(model, epochs=20, steps_per_epoch=500):\n",
        "    model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data.repeat(),\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_data,\n",
        "        validation_steps=50,\n",
        "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
        "    )\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "rsn7AQnX9pyJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_translator, history = compile_and_train(translator)"
      ],
      "metadata": {
        "id": "ox4XKJEe9zMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c4eae0-94b0-42b1-f074-65ff931dbcb8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 62s 86ms/step - loss: 5.1520 - masked_acc: 0.2187 - masked_loss: 5.1539 - val_loss: 4.2751 - val_masked_acc: 0.3475 - val_masked_loss: 4.2762\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 27s 55ms/step - loss: 3.6889 - masked_acc: 0.4199 - masked_loss: 3.6903 - val_loss: 3.0407 - val_masked_acc: 0.4972 - val_masked_loss: 3.0425\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 29s 57ms/step - loss: 2.7443 - masked_acc: 0.5434 - masked_loss: 2.7456 - val_loss: 2.4165 - val_masked_acc: 0.5792 - val_masked_loss: 2.4172\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 27s 55ms/step - loss: 2.2278 - masked_acc: 0.6152 - masked_loss: 2.2281 - val_loss: 1.9886 - val_masked_acc: 0.6422 - val_masked_loss: 1.9890\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 1.8850 - masked_acc: 0.6643 - masked_loss: 1.8861 - val_loss: 1.7248 - val_masked_acc: 0.6837 - val_masked_loss: 1.7255\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 1.6348 - masked_acc: 0.6960 - masked_loss: 1.6363 - val_loss: 1.6023 - val_masked_acc: 0.7004 - val_masked_loss: 1.6018\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 27s 55ms/step - loss: 1.4939 - masked_acc: 0.7157 - masked_loss: 1.4947 - val_loss: 1.4743 - val_masked_acc: 0.7153 - val_masked_loss: 1.4747\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 1.4159 - masked_acc: 0.7262 - masked_loss: 1.4166 - val_loss: 1.3876 - val_masked_acc: 0.7271 - val_masked_loss: 1.3892\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 1.3252 - masked_acc: 0.7391 - masked_loss: 1.3261 - val_loss: 1.3218 - val_masked_acc: 0.7353 - val_masked_loss: 1.3219\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 27s 55ms/step - loss: 1.1949 - masked_acc: 0.7537 - masked_loss: 1.1962 - val_loss: 1.2326 - val_masked_acc: 0.7527 - val_masked_loss: 1.2336\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 1.0865 - masked_acc: 0.7673 - masked_loss: 1.0870 - val_loss: 1.1859 - val_masked_acc: 0.7593 - val_masked_loss: 1.1861\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 1.0806 - masked_acc: 0.7695 - masked_loss: 1.0812 - val_loss: 1.1440 - val_masked_acc: 0.7581 - val_masked_loss: 1.1443\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 28s 55ms/step - loss: 1.0427 - masked_acc: 0.7734 - masked_loss: 1.0436 - val_loss: 1.1326 - val_masked_acc: 0.7602 - val_masked_loss: 1.1336\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 27s 55ms/step - loss: 1.0322 - masked_acc: 0.7746 - masked_loss: 1.0333 - val_loss: 1.1115 - val_masked_acc: 0.7635 - val_masked_loss: 1.1121\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 0.9081 - masked_acc: 0.7924 - masked_loss: 0.9092 - val_loss: 1.0809 - val_masked_acc: 0.7693 - val_masked_loss: 1.0826\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 0.8759 - masked_acc: 0.7951 - masked_loss: 0.8764 - val_loss: 1.0695 - val_masked_acc: 0.7727 - val_masked_loss: 1.0710\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 0.8721 - masked_acc: 0.7971 - masked_loss: 0.8731 - val_loss: 1.0192 - val_masked_acc: 0.7769 - val_masked_loss: 1.0205\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 27s 54ms/step - loss: 0.8771 - masked_acc: 0.7955 - masked_loss: 0.8781 - val_loss: 1.0181 - val_masked_acc: 0.7800 - val_masked_loss: 1.0195\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 26s 52ms/step - loss: 0.8616 - masked_acc: 0.7974 - masked_loss: 0.8626 - val_loss: 0.9907 - val_masked_acc: 0.7832 - val_masked_loss: 0.9907\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 28s 55ms/step - loss: 0.7451 - masked_acc: 0.8157 - masked_loss: 0.7461 - val_loss: 0.9900 - val_masked_acc: 0.7832 - val_masked_loss: 0.9910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inference"
      ],
      "metadata": {
        "id": "kyimttWj97fK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The model trained is used for inference.\n",
        "\n",
        "This function is meant to be used inside a for-loop, so you feed to it the information of the previous step to generate the information of the next step"
      ],
      "metadata": {
        "id": "zvdCjl5i-AZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_token(decoder, context, next_token, done, state, temperature=0.0):\n",
        "\n",
        "    logits, state = decoder(context, next_token, state=state, return_state=True)\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "    # If temp is 0 then next_token is the argmax of logits\n",
        "    if temperature == 0.0:\n",
        "        next_token = tf.argmax(logits, axis=-1)\n",
        "\n",
        "    # If temp is not 0 then next_token is sampled out of logits\n",
        "    else:\n",
        "        logits = logits / temperature\n",
        "        next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "    logits = tf.squeeze(logits)\n",
        "    next_token = tf.squeeze(next_token)\n",
        "    logit = logits[next_token].numpy()\n",
        "\n",
        "    # Reshape to (1,1) since this is the expected shape for text encoded as TF tensors\n",
        "    next_token = tf.reshape(next_token, shape=(1,1))\n",
        "\n",
        "    # If next_token is End-of-Sentence token you are done\n",
        "    if next_token == eos_id:\n",
        "        done = True\n",
        "\n",
        "    return next_token, logit, state, done"
      ],
      "metadata": {
        "id": "JJkyNHqs-KsU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_sentence = \"I love languages\"\n",
        "texts = tf.convert_to_tensor(eng_sentence)[tf.newaxis]\n",
        "# Vectorize it and pass it through the encoder\n",
        "context = english_vectorizer(texts).to_tensor()\n",
        "context = encoder(context)\n",
        "\n",
        "next_token = tf.fill((1,1), sos_id)\n",
        "\n",
        "state = [tf.random.uniform((1, UNITS)), tf.random.uniform((1, UNITS))]\n",
        "done = False\n",
        "\n",
        "next_token, logit, state, done = generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
        "print(f\"Next token: {next_token}\\nLogit: {logit:.4f}\\nDone? {done}\")"
      ],
      "metadata": {
        "id": "qQCrWzSK-RAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfd4f2a-fed6-42fa-9c8c-d92443688c8a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token: [[6365]]\n",
            "Logit: -18.7164\n",
            "Done? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation"
      ],
      "metadata": {
        "id": "CYZePA90-ZnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This function will take care of the following steps:\n",
        "\n",
        "- Process the sentence to translate and encode it\n",
        "\n",
        "+ Set the initial state of the decoder\n",
        "\n",
        "- Get predictions of the next token (starting with the \\<SOS> token) for a maximum of iterations (in case the \\<EOS> token is never returned)\n",
        "    \n",
        "+ Return the translated text (as a string), the logit of the last iteration (this helps measure how certain was that the sequence was translated in its totality) and the translation in token format.\n",
        "\n"
      ],
      "metadata": {
        "id": "ge-a2vse-d7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, text, max_length=50, temperature=0.0):\n",
        "    tokens, logits = [], []\n",
        "    text = tf.constant([text])\n",
        "    # Vectorize the text using the correct vectorizer\n",
        "    context = english_vectorizer(text).to_tensor()\n",
        "    context = model.encoder(context)\n",
        "    next_token = tf.constant([[2]])\n",
        "\n",
        "    state = [tf.zeros((1,UNITS)), tf.zeros((1,UNITS))]\n",
        "\n",
        "    done = False\n",
        "\n",
        "    # Iterate for max_length iterations\n",
        "    for _ in range(max_length):\n",
        "        next_token, logit, state, done =generate_next_token(decoder, context, next_token, done, state, temperature=0.5)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Add next_token to the list of tokens\n",
        "    tokens.append(next_token)\n",
        "\n",
        "    # Add logit to the list of logits\n",
        "    logits.append(logit)\n",
        "\n",
        "    # Concatenate all tokens into a tensor\n",
        "    tokens = tf.concat(tokens, axis=-1)\n",
        "\n",
        "    translation = tf.squeeze(tokens_to_text(tokens, id_to_word))\n",
        "    translation = translation.numpy().decode()\n",
        "\n",
        "    return translation, logits[-1], tokens"
      ],
      "metadata": {
        "id": "WE8EpVK8-iMq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "temp = 0.0\n",
        "original_sentence = \"I love languages\"\n",
        "\n",
        "translation, logit, tokens = translate(trained_translator, original_sentence, temperature=temp)\n",
        "\n",
        "print(f\"Temperature: {temp}\\n\\nOriginal sentence: {original_sentence}\\nTranslation: {translation}\\nTranslation tokens:{tokens}\\nLogit: {logit:.3f}\")"
      ],
      "metadata": {
        "id": "LI8svx4l_aky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b53c8f6-1255-4197-b1f7-5c4595e8d29c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature: 0.0\n",
            "\n",
            "Original sentence: I love languages\n",
            "Translation: desmontar\n",
            "Translation tokens:[[6923]]\n",
            "Logit: -18.764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Minimum Bayes-Risk Decoding\n",
        "\n",
        "Getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR.\n",
        "\n",
        " The general steps to implement this are:\n",
        "\n",
        "- Take several random samples\n",
        "+ Score each sample against all other samples\n",
        "- Select the one with the highest score\n"
      ],
      "metadata": {
        "id": "2UDrV0CC_vdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function will return any desired number of candidate translations alongside the log-probability for each one\n",
        "def generate_samples(model, text, n_samples=4, temperature=0.6):\n",
        "\n",
        "    samples, log_probs = [], []\n",
        "    for _ in range(n_samples):\n",
        "\n",
        "        _, logp, sample = translate(model, text, temperature=temperature)\n",
        "\n",
        "        # Save the translated tensors\n",
        "        samples.append(np.squeeze(sample.numpy()).tolist())\n",
        "\n",
        "        # Save the logits\n",
        "        log_probs.append(logp)\n",
        "\n",
        "    return samples, log_probs"
      ],
      "metadata": {
        "id": "KZnXhYV4_zKL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples, log_probs = generate_samples(trained_translator, 'I love languages')\n",
        "\n",
        "for s, l in zip(samples, log_probs):\n",
        "    print(f\"Translated tensor: {s} has logit: {l:.3f}\")"
      ],
      "metadata": {
        "id": "Y0Jo9aUM_3Yj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777b0be7-e4fb-4855-ebef-2308b60eb981"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated tensor: 2576 has logit: -18.799\n",
            "Translated tensor: 7195 has logit: -18.798\n",
            "Translated tensor: 10243 has logit: -18.781\n",
            "Translated tensor: 1336 has logit: -18.772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Scores\n",
        "\n",
        "\n",
        "In order to evaluate multiple translations effectively, it's essential to establish a method for measuring their quality. One approach is to compare each translation sample with the others.\n",
        "\n",
        "Several metrics, particularly focusing on unigram overlaps, can aid in this assessment."
      ],
      "metadata": {
        "id": "YclceOncAGZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jaccard Similarity\n",
        "\n",
        "One such metric is the widely used Jaccard similarity, which calculates the intersection over union of two sets. The jaccard_similarity function is dedicated to providing this metric for any pair of candidate and reference translations"
      ],
      "metadata": {
        "id": "tslO-PEj3Er8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "    candidate_set = set(candidate)\n",
        "    reference_set = set(reference)\n",
        "    common_tokens = candidate_set.intersection(reference_set)\n",
        "    all_tokens = candidate_set.union(reference_set)\n",
        "    overlap = len(common_tokens) / len(all_tokens)\n",
        "\n",
        "    return overlap"
      ],
      "metadata": {
        "id": "9eZZZ4qtAHSC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### rouge1_similarity\n",
        "\n",
        "Jaccard similarity is good but a more commonly used metric in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 you can output the scores for both precision and recall when comparing two samples.\n",
        "\n",
        "To get the final score, compute the F1-score as given by:\n",
        "\n",
        "\n",
        "\n",
        "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$\n"
      ],
      "metadata": {
        "id": "VYTRjmVkAnnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rouge1_similarity(candidate, reference):\n",
        "    candidate = str(candidate)\n",
        "    reference = str(reference)\n",
        "    candidate_word_counts = Counter(candidate)\n",
        "    reference_word_counts = Counter(reference)\n",
        "\n",
        "    overlap = 0\n",
        "    for token in candidate_word_counts.keys():\n",
        "        token_count_candidate = candidate_word_counts[token]\n",
        "        token_count_reference = reference_word_counts[token]\n",
        "\n",
        "        overlap += min(token_count_candidate, token_count_reference)\n",
        "    precision = overlap / len(candidate)\n",
        "\n",
        "    recall = overlap / len(reference)\n",
        "\n",
        "    if precision + recall != 0:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "        return f1_score\n",
        "\n",
        "\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "GEXDj7gtAm-9"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing the Overall Score\n",
        "\n",
        " the steps to generate the scores of a 4-sample list.\n",
        "\n",
        "- Get similarity score between sample 1 and sample 2\n",
        "+ Get similarity score between sample 1 and sample 3\n",
        "- Get similarity score between sample 1 and sample 4\n",
        "+ Get average score of the first 3 steps. This will be the overall score of sample 1\n",
        "- Iterate and repeat until samples 1 to 4 have overall scores.\n",
        "\n",
        "\n",
        "The results will be stored in a dictionary for easy lookups.\n",
        "\n"
      ],
      "metadata": {
        "id": "ffaejmAGA8mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_overlap(samples, similarity_fn):\n",
        "    scores = {}\n",
        "\n",
        "    # Iterate through all samples\n",
        "    for index_candidate, candidate in enumerate(samples):\n",
        "\n",
        "        overlap = 0\n",
        "\n",
        "        for index_sample, sample in enumerate(samples):\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "\n",
        "            # Get the overlap between candidate and sample using the similarity function\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "\n",
        "\n",
        "            overlap += sample_overlap\n",
        "\n",
        "        score = overlap / (len(samples) - 1)\n",
        "        score = round(score, 3)\n",
        "        scores[index_candidate] = score\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "DRPVVqLHA9dv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_avg_overlap(samples, log_probs, similarity_fn):\n",
        "    scores = {}\n",
        "    for index_candidate, candidate in enumerate(samples):\n",
        "        overlap, weight_sum = 0.0, 0.0\n",
        "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "\n",
        "            sample_p = float(np.exp(logp))\n",
        "            weight_sum += sample_p\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "            overlap += sample_p * sample_overlap\n",
        "\n",
        "        score = overlap / weight_sum\n",
        "        score = round(score, 3)\n",
        "        scores[index_candidate] = score\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "N7jjfJ_PBG9H"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mbr_decode\n"
      ],
      "metadata": {
        "id": "FMXywkpMBQJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mbr_decode(model, text, n_samples=5, temperature=0.6, similarity_fn=rouge1_similarity):\n",
        "    samples, log_probs = generate_samples(model, text, n_samples=n_samples, temperature=temperature)\n",
        "    scores = weighted_avg_overlap(samples, log_probs, similarity_fn)\n",
        "    decoded_translations = [tokens_to_text(s, id_to_word).numpy().decode('utf-8') for s in samples]\n",
        "\n",
        "    # Find the key with the highest score\n",
        "    max_score_key = max(scores, key=lambda k: scores[k])\n",
        "    translation = decoded_translations[max_score_key]\n",
        "\n",
        "    return translation, decoded_translations"
      ],
      "metadata": {
        "id": "ArOTIwUIBQ5i"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentence = \"I love languages\"\n",
        "\n",
        "translation, candidates = mbr_decode(trained_translator, english_sentence, n_samples=10, temperature=0.6)\n",
        "\n",
        "print(\"Translation candidates:\")\n",
        "for c in candidates:\n",
        "    print(c)\n",
        "\n",
        "print(f\"\\nSelected translation: {translation}\")"
      ],
      "metadata": {
        "id": "tMtXHSmMBUO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de77fed4-e7c0-4b8d-bd91-1fbd3dbc8e70"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected translation: eu amo línguas\n"
          ]
        }
      ]
    }
  ]
}